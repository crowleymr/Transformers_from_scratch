{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a GPT model from scratch using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sources:\n",
    "* https://github.com/karpathy/nanoGPT\n",
    "* [GPT Model Behind the Scene: Exploring it from scratch with Pytorch](https://ai.plainenglish.io/creating-and-exploring-gpt-from-scratch-ffe84ac415a9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# import inspect\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "# from typing import Tuple\n",
    "from typing import Optional\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import os\n",
    "import time\n",
    "# import pickle\n",
    "# from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "import fitz\n",
    "import zipfile\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import requests\n",
    "import tiktoken\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import random\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pre-train custom GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=Path(os.getcwd())\n",
    "data_path=root_path.joinpath('data')\n",
    "corpus_path=data_path.joinpath('corpus')\n",
    "pdf_path=data_path.joinpath('apra_pdfs')\n",
    "model_path=root_path.joinpath('model')\n",
    "\n",
    "Path(data_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(corpus_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(pdf_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(model_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Split shakespeare into 20 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_01.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_02.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_03.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_04.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_05.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_06.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_07.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_08.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_09.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_10.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_11.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_12.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_13.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_14.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_15.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_16.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_17.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_18.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_19.txt\n",
      "Written: d:\\repos\\Transformers_from_scratch\\data\\corpus\\shakespear_20.txt\n"
     ]
    }
   ],
   "source": [
    "def split_file(input_filename: str,\n",
    "               source_path: Path,\n",
    "               target_path: Path,\n",
    "               num_chunks: Optional[int]=20\n",
    "               ):\n",
    "    \n",
    "    # Read all the lines from the input file\n",
    "    with open(source_path.joinpath(input_filename), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate the number of lines per chunk\n",
    "    total_lines = len(lines)\n",
    "    lines_per_chunk = total_lines // num_chunks\n",
    "    remainder = total_lines % num_chunks  # For uneven splits\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Calculate start and end indices for each chunk\n",
    "        start = i * lines_per_chunk + min(i, remainder)\n",
    "        end = start + lines_per_chunk + (1 if i < remainder else 0)\n",
    "\n",
    "        # Generate a chunk filename\n",
    "        chunk_filename = target_path.joinpath(f\"{input_filename[:-4]}_{i+1:02}.txt\")\n",
    "\n",
    "        # Write the current chunk to a new file\n",
    "        with open(chunk_filename, 'w') as chunk_file:\n",
    "            chunk_file.writelines(lines[start:end])\n",
    "\n",
    "        print(f\"Written: {chunk_filename}\")\n",
    "        \n",
    "split_file('shakespear.txt',data_path,corpus_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3) Unzip pdfs and convert them to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 'APRA_Guidelines.zip' to 'd:\\repos\\Transformers_from_scratch\\data\\apra_pdfs'.\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\3PS 222 Intra-group Transactions and Exposures.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APG 223 Residential Mortgage Lending.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 110 Capital Adequacy.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 112 Capital Adequacy Standardised Approach.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 113 Capital Adequacy Internal Ratings-based.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 180 Capital Adequacy Counterparty Credit Risk.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 210 Liquidity.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 220 Credit Risk Management.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\APS 222 Associations with Related Entities.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\ARS 118 Off-balance Sheet Business.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\ARS 230 Commercial Property.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\ARS 322 Statement of Financial Position.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPG 229 Climate Change Financial Risks.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPG-235-Managing-Data-Risk.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPS 226 Margining and risk mitigation for non-c.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPS 230 Operational Risk Management.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPS 231 Outsourcing.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPS 232 Business Continuity.txt\n",
      "Created d:\\repos\\Transformers_from_scratch\\data\\corpus\\CPS 234 Information Security.txt\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(data_path.joinpath('APRA_Guidelines.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(pdf_path)\n",
    "    print(f\"Extracted 'APRA_Guidelines.zip' to '{pdf_path}'.\")\n",
    "\n",
    "def pdf_to_text(pdf_path: Path, txt_path: Path):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    # Create a text file to store the extracted text\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        for page_number in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_number)\n",
    "            text = page.get_text()\n",
    "            text_file.write(text)\n",
    "\n",
    "    # Close the PDF\n",
    "    pdf_document.close()\n",
    "\n",
    "def convert_pdfs_in_folder(source_path: Path, target_path: Path):\n",
    "    for file in os.listdir(source_path):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_to_text(source_path.joinpath(file), target_path.joinpath(re.sub('.pdf','.txt',file)))\n",
    "            print(f'Created {target_path.joinpath(re.sub('.pdf','.txt',file))}')\n",
    "\n",
    "convert_pdfs_in_folder(pdf_path,corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4) Split corpus into train, test and validation sets\n",
    "Splitting was later found to be unnecessary, as full dataset will be used to perform unsupervised training. Validation will come later in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [corpus_path.joinpath(file) for file in os.listdir(corpus_path) if file.endswith('.txt')]\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "# random_seed=1986\n",
    "# random.seed(random_seed)\n",
    "# train_files, val_files = train_test_split(file_list, test_size=0.2, random_state=random_seed)\n",
    "train_files = file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "Documents in training corpus\n",
      "===================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/3PS 222 Intra-group Transactions and Exposures.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APG 223 Residential Mortgage Lending.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 110 Capital Adequacy.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 112 Capital Adequacy Standardised Approach.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 113 Capital Adequacy Internal Ratings-based.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 180 Capital Adequacy Counterparty Credit Risk.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 210 Liquidity.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 220 Credit Risk Management.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/APS 222 Associations with Related Entities.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/ARS 118 Off-balance Sheet Business.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/ARS 230 Commercial Property.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/ARS 322 Statement of Financial Position.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPG 229 Climate Change Financial Risks.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPG-235-Managing-Data-Risk.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPS 226 Margining and risk mitigation for non-c.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPS 230 Operational Risk Management.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPS 231 Outsourcing.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPS 232 Business Continuity.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/CPS 234 Information Security.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_01.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_02.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_03.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_04.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_05.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_06.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_07.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_08.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_09.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_10.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_11.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_12.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_13.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_14.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_15.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_16.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_17.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_18.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_19.txt'),\n",
       " WindowsPath('d:/repos/Transformers_from_scratch/data/corpus/shakespear_20.txt')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('===================================================')\n",
    "print(\"Documents in training corpus\")\n",
    "print('===================================================')\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('===================================================')\n",
    "# print(\"Documents in validation corpus\")\n",
    "# print('===================================================')\n",
    "# val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5) Concatenate individual files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text has been written to: d:\\repos\\Transformers_from_scratch\\data\\train.txt\n"
     ]
    }
   ],
   "source": [
    "def concatenate_files_to_output(file_list, output_file):\n",
    "    \"\"\"\n",
    "    Concatenate text from multiple files and write to a single output file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_list: List of paths to the input text files.\n",
    "    - output_file: Path to the output text file where combined text will be saved.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for file_path in file_list:\n",
    "            # Ensure the file exists\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                    # Read the contents of the file and write to output file\n",
    "                    outfile.write(infile.read() + \"\\n\")  # Add a newline to separate contents\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "train_text_path = data_path.joinpath('train.txt')\n",
    "concatenate_files_to_output(train_files, train_text_path)\n",
    "print(f\"Combined text has been written to: {train_text_path}\")\n",
    "\n",
    "# No longer needed\n",
    "# val_text_path = data_path.joinpath('val.txt')\n",
    "# concatenate_files_to_output(val_files, val_text_path)\n",
    "# print(f\"Combined text has been written to: {val_text_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6) Apply GPT-2 byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt has 618,255 tokens\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def apply_gpt2_byte_encoding(data_path, source_file, target_file):\n",
    "    with open(data_path.joinpath(source_file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Some low level text cleaning\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text.strip())\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "        # encode with tiktoken gpt2 bpe\n",
    "        encoded_text = enc.encode_ordinary(text)\n",
    "        print(f\"{source_file} has {len(encoded_text):,} tokens\")\n",
    "        encoded_text = np.array(encoded_text, dtype=np.uint16)\n",
    "        encoded_text.tofile(data_path.joinpath(target_file))\n",
    "        return len(encoded_text)\n",
    "\n",
    "train_bte_path = data_path.joinpath('train.bin')\n",
    "# val_bte_path   = data_path.joinpath('val.bin')\n",
    "\n",
    "train_vocab_size=apply_gpt2_byte_encoding(data_path,'train.txt','train.bin')\n",
    "# val_vocab_size=apply_gpt2_byte_encoding(data_path,'val.txt','val.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "class CustomConfig(GPTConfig):\n",
    "    n_layer = 8\n",
    "    n_head = 8\n",
    "    n_embd = 256\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    dropout = 0.1\n",
    "    compile = False\n",
    "    device = 'cuda'\n",
    "    num_workers = 0\n",
    "    max_iters = 2e4\n",
    "    batch_size = 4\n",
    "    block_size = 128\n",
    "    learning_rate = 6e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    weight_decay = 1e-1\n",
    "    grad_norm_clip = 1.0\n",
    "    bias = True\n",
    "\n",
    "config = CustomConfig(vocab_size=train_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Dataset constructor and Dataloader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .bin\n",
    "train_data = np.memmap(train_bte_path, dtype=np.uint16, mode='r')\n",
    "# val_data   = np.memmap(val_bte_path, dtype=np.uint16, mode='r')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, split, block_size=128, device_type='cuda'):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.device_type = device_type\n",
    "        # self.data = train_data if split == 'train' else val_data\n",
    "        # Removed option for val_data\n",
    "        self.data = train_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx : idx + self.block_size].astype(np.int64))\n",
    "        y = torch.from_numpy(self.data[idx + 1 : idx + 1 + self.block_size].astype(np.int64)) \n",
    "\n",
    "        if self.device_type == 'cuda':\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y = x.pin_memory().to('cuda', non_blocking=True), y.pin_memory().to('cuda', non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to('cpu'), y.to('cpu')\n",
    "        return x, y\n",
    "\n",
    "# create dataset and dataloader\n",
    "train_dataset = TextDataset('train', config.block_size, config.device)\n",
    "\n",
    "# Trainer() constructor does not use this data loader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n",
    "\n",
    "# val_dataset = TextDataset('val', config.block_size, config.device)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1) Causal Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.functional.scaled_dot_product_attention** is equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matts_scaled_dot_product_attention(query:     Tensor,\n",
    "                                       key:       Tensor,\n",
    "                                       value:     Tensor,\n",
    "                                       mask:      Optional[Tensor]=None,\n",
    "                                       dropout_p: Optional[float]=0.0,\n",
    "                                       is_causal: Optional[bool]=False,\n",
    "                                       scale:     Optional[float]=None\n",
    "                                       ) -> Tensor:\n",
    "    \"\"\"\n",
    "    Combine three tensors; query, key, and value; to generate an output tensor of scaled dot product attention.\n",
    "\n",
    "    Parameters:\n",
    "    - query (Tensor)              - shape (N x ... x L x E)\n",
    "    - key (Tensor)                - shape (N x ... x S x E)\n",
    "    - value (Tensor)              - shape (N x ... x S x Ev)\n",
    "    - mask (optional Tensor)      - shape (N x ... x L x S)\n",
    "\n",
    "            mask; shape must be broadcastable to the shape of attention weights.\n",
    "                            Two types of masks are supported.\n",
    "                                1) A boolean mask where a value of True indicates that the element should take part in attention.\n",
    "                                2) A float mask of the same type as query, key, value that is added to the attention score.\n",
    "\n",
    "    - dropout_p (float)           - Dropout probability; if greater than 0.0, dropout is applied\n",
    "\n",
    "    Returns:\n",
    "    - Attention output (Tensor)   - shape (N x ... x L x Ev)\n",
    "\n",
    "\n",
    "    Shape legend:\n",
    "    - N:    Batch size\n",
    "    - ...:  Any number of other batch dimensions (optional)\n",
    "    - S:    Source sequence length\n",
    "    - L:    Target sequence length\n",
    "    - E:    Embedding dimension of the query and key\n",
    "    - Ev:   Embedding dimension of the value\n",
    "    \"\"\"\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "\n",
    "    # Calculate scaling factor ahead of time\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "\n",
    "    # Pre-define attn_bias as zero-weighted tensor\n",
    "    # this allows it to be included in the attn_weight\n",
    "    # calculation regardless of being defined\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "\n",
    "    if is_causal:\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if mask is not None:\n",
    "        if mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += mask\n",
    "\n",
    "    # Compute attention weights\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "\n",
    "    # Apply softmax to the attention weights\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "\n",
    "    # Apply dropout if specified\n",
    "    if dropout_p > 0:\n",
    "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "\n",
    "    # Compute the final output\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection \n",
    "    at the end.\n",
    "    It's important in decoder block to have diagonal mask\n",
    "    It is also possible to use torch.nn.MultiheadAttention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.dropout = config.dropout\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(\n",
    "                        torch.nn.functional, \n",
    "                        'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\n",
    "              \"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\n",
    "                \"mask\", \n",
    "                torch.tril(torch.ones(config.block_size, config.block_size)\n",
    "            ).view(1, 1, config.block_size, config.block_size))\n",
    "            \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # batch_size, seq_len, emb_dim\n",
    "        B, T, C = x.size() \n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # (b, seq_len, emb_dim) --> (b, seq_len, emb_dim * 3) --> (b, seq_len, emb_dim)\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
    "        \n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "\n",
    "                # Hack to excluding use of attention_mask as it only leads to problems when using transformers data loader and trainer\n",
    "                # q, k, v, attn_mask=attention_mask, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
    "            )\n",
    "        else:\n",
    "            # (b, h, seq_len, d_k) matmul (b, h, d_k, seq_len) --> (b, h, seq_len, seq_len)\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            # diagonal mask\n",
    "            # fill 0 mask with super small number so it wont affect the softmax weight\n",
    "            # (batch_size, h, seq_len, seq_len)\n",
    "            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "\n",
    "            # (b, h, seq_len, seq_len) matmul (b, h, seq_len, d_k) --> (b, h, seq_len, d_k)\n",
    "            y = att @ v \n",
    "\n",
    "        # (b, h, seq_len, d_k) --> (b, seq_len, h, d_k) --> (b, seq_len, d_model)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2) Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom GELU class to match Google BERT repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer perceptron model that processes information from the Multi-head Attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = NewGELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" GPT decoder block\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = x + self.attn(self.ln_1(x), attention_mask)\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3) GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        \n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, targets=None):\n",
    "        device = input_ids.device\n",
    "        b, t = input_ids.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "\n",
    "        # positional token, shape (1, t)\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) \n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(input_ids) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, attention_mask)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        # (b, t, n_embd) -- > # (b, t, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        # -1 at output will be ignored\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b, t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # either sample from the distribution or take the most likely element\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5) Trainer Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, config, model, train_dataset):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_dataset = train_dataset\n",
    "        self.callbacks = defaultdict(list)\n",
    "        self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config)\n",
    "\n",
    "        # setup the dataloader\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
    "            shuffle=False,\n",
    "            # pin_memory=True,\n",
    "            batch_size=config.batch_size,\n",
    "            num_workers=config.num_workers,\n",
    "        )\n",
    "\n",
    "        model.train()\n",
    "        self.iter_num = 0\n",
    "        self.iter_time = time.time()\n",
    "        data_iter = iter(train_loader)\n",
    "        while True:\n",
    "\n",
    "            # fetch the next batch (x, y) and re-init iterator if needed\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            x, y = batch\n",
    "\n",
    "            # forward the model\n",
    "            logits, self.loss = model(input_ids=x, targets=y)\n",
    "\n",
    "            # backprop and update the parameters\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            self.loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.trigger_callbacks('on_batch_end')\n",
    "            self.iter_num += 1\n",
    "            tnow = time.time()\n",
    "            self.iter_dt = tnow - self.iter_time\n",
    "            self.iter_time = tnow\n",
    "\n",
    "            # termination conditions\n",
    "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
    "                torch.cuda.empty_cache()\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6) Execute model pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 164.62M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crowl\\AppData\\Local\\Temp\\ipykernel_14696\\2197486765.py:51: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 13.41142\n",
      "iter_dt 203.05ms; iter 500: train loss 5.78267\n",
      "iter_dt 202.80ms; iter 1000: train loss 5.41902\n",
      "iter_dt 214.38ms; iter 1500: train loss 4.79306\n",
      "iter_dt 201.72ms; iter 2000: train loss 4.59108\n",
      "iter_dt 204.96ms; iter 2500: train loss 4.67543\n",
      "iter_dt 202.17ms; iter 3000: train loss 4.43193\n",
      "iter_dt 213.41ms; iter 3500: train loss 4.40250\n",
      "iter_dt 199.89ms; iter 4000: train loss 3.94002\n",
      "iter_dt 215.62ms; iter 4500: train loss 3.00203\n",
      "iter_dt 201.11ms; iter 5000: train loss 3.60044\n",
      "iter_dt 204.08ms; iter 5500: train loss 4.29090\n",
      "iter_dt 204.07ms; iter 6000: train loss 3.82599\n",
      "iter_dt 203.79ms; iter 6500: train loss 3.80347\n",
      "iter_dt 198.82ms; iter 7000: train loss 3.92197\n",
      "iter_dt 214.21ms; iter 7500: train loss 3.60690\n",
      "iter_dt 203.65ms; iter 8000: train loss 3.34533\n",
      "iter_dt 202.43ms; iter 8500: train loss 4.37069\n",
      "iter_dt 204.28ms; iter 9000: train loss 4.18926\n",
      "iter_dt 202.53ms; iter 9500: train loss 4.16536\n",
      "iter_dt 211.23ms; iter 10000: train loss 3.18089\n",
      "iter_dt 205.73ms; iter 10500: train loss 3.04692\n",
      "iter_dt 217.11ms; iter 11000: train loss 3.39755\n",
      "iter_dt 202.09ms; iter 11500: train loss 3.34237\n",
      "iter_dt 203.33ms; iter 12000: train loss 3.51903\n",
      "iter_dt 202.47ms; iter 12500: train loss 3.92739\n",
      "iter_dt 202.40ms; iter 13000: train loss 3.42203\n",
      "iter_dt 199.98ms; iter 13500: train loss 3.68711\n",
      "iter_dt 203.39ms; iter 14000: train loss 3.64258\n",
      "iter_dt 203.70ms; iter 14500: train loss 3.37648\n",
      "iter_dt 202.44ms; iter 15000: train loss 2.64041\n",
      "iter_dt 203.43ms; iter 15500: train loss 4.19646\n",
      "iter_dt 201.77ms; iter 16000: train loss 3.46516\n",
      "iter_dt 204.41ms; iter 16500: train loss 3.29972\n",
      "iter_dt 203.55ms; iter 17000: train loss 3.08013\n",
      "iter_dt 201.73ms; iter 17500: train loss 3.65008\n",
      "iter_dt 219.82ms; iter 18000: train loss 3.96245\n",
      "iter_dt 216.54ms; iter 18500: train loss 3.17632\n",
      "iter_dt 201.16ms; iter 19000: train loss 2.64157\n",
      "iter_dt 202.67ms; iter 19500: train loss 3.17500\n"
     ]
    }
   ],
   "source": [
    "model_mattGPT = GPT(config).to(config.device)\n",
    "if config.compile:\n",
    "     model_mattGPT = torch.compile(model_mattGPT)\n",
    "trainer = Trainer(config, model_mattGPT, train_dataset)\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 500 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model_mattGPT,model_path.joinpath('model_mattGPT_1.pkl'))\n",
    "# torch.save(model_mattGPT,model_path.joinpath('model_mattGPT.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7) Initial look at generated text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model if running from new session\n",
    "if 'model_mattGPT' not in globals():\n",
    "    model_path=Path(os.getcwd()).joinpath('model')\n",
    "    model_mattGPT = torch.load(model_path.joinpath('model_mattGPT_1.pkl'), weights_only=False)\n",
    "\n",
    "def print_mattGPT(prompt):\n",
    "    sample_ids = torch.Tensor(enc.encode_ordinary(prompt)).long()\n",
    "    sample_ids = torch.unsqueeze(sample_ids, 0).to(config.device)\n",
    "    result = model_mattGPT.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n",
    "    print(enc.decode(result.detach().cpu().tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shakespearian language prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord:\n",
      "Rise! My people, conquer the north!\n",
      "RIVERS:\n",
      "I thank you not, sir; I have you a little word,\n",
      "And I will be so still my wife.\n",
      "KING RICHARD III:\n",
      "I am a man, sir, that I may live to die.\n",
      "PETRUCHIO:\n",
      "I thank you, sir; I beseech you, sir.\n",
      "LADY ANNE:\n",
      "I am a kind of kings, that I must die.\n",
      "KING RICHARD III:\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('Lord:\\nRise! My people, conquer the north!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APRA Banking regulation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An ADI must have an independent credit risk control unit that is responsible for \n",
      "the ADI to use the internal assessment and approval process for determining the \n",
      "ADIs credit risk management strategy and risk management. \n",
      "The valuation process must be documented and documented and documented and appropriate \n",
      "to the ADIs credit risk\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('An ADI must have an independent credit risk control unit that is responsible for')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed shakespearian language + banking regulation prompt - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING RICHARD III:\n",
      " The key requirements of this Prudential Standard are that Montague hath breathed his last loan-to-value ratio.\n",
      "RIVERS:\n",
      "I think the king is notional amount of all the ADIs:\n",
      "And I, that I mean the king.\n",
      "KING RICHARD III:\n",
      "I hope the king, that was the king, and the\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('KING RICHARD III:\\n The key requirements of this Prudential Standard are that Montague hath breathed his last loan-to-value ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed shakespearian language + banking regulation prompt - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "We are accounted poor citizens and believe that special purpose vehicles (SPVs) holding securitised assets may be \n",
      "eligible for regulatory capital and APS 120. \n",
      "Authorised Version F2022L01578 registered 05/12/2022\n",
      "January 2023 \n",
      "CPS 230.0 - 3 \n",
      "Prudential Standard APS\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('First Citizen:\\nWe are accounted poor citizens and believe that special purpose vehicles (SPVs) holding securitised assets may be')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Seuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not eat green eggs and ham her,\n",
      "And I will be married to her.\n",
      "I am a kind of kings,\n",
      "And, when my name was not a fool!\n",
      "I, that I was, that I was,\n",
      "I was the man that I had been\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('I do not eat green eggs and ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Star Wars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luke, I am your father.\n",
      "First Lord:\n",
      "I am a man that I have to die, and\n",
      "I am a man of you.\n",
      "First Gentleman:\n",
      "I am a kind of kings, which\n",
      "say is a man that I was,\n",
      "and a\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('Luke, I am your father')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Philosophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth is in the eye\n",
      "Of the world, and the king,\n",
      "And all the king, the king, and the king,\n",
      "And his son, his son, his son, his son,\n",
      "And his son, his son, his son, his son,\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('Truth is in the eye')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newtons third law states that he hath\n",
      "His oath to be a king.\n",
      "BUCKINGHAM:\n",
      "My lord, I beseech you,\n",
      "I may not be so, my lord.\n",
      "BUCKINGHAM:\n",
      "My lord, I will not be\n"
     ]
    }
   ],
   "source": [
    "print_mattGPT('Newtons third law states that')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Fine-tune model and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* [Fine-Tuning GPT-2 for Sentiment Analysis](https://drlee.io/fine-tuning-gpt-2-for-sentiment-analysis-94ebdd7b5b24)\n",
    "* [Train and Deploy Fine-Tuned GPT-2 Model Using PyTorch on Amazon SageMaker to Classify News Articles](https://towardsdatascience.com/train-and-deploy-fine-tuned-gpt-2-model-using-pytorch-on-amazon-sagemaker-to-classify-news-articles-612f9957c7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Attempt to test custom GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Download imdb and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Tokenise IMBD dataset using GPT-2 byte-pair encoding\n",
    "Customised the block size to the custom GPT size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.block_size)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3) Add output linear layer to custom GPT\n",
    "\n",
    "The below code was sourced from Chat GPT using several very detailed prompts, all documented in the history link below.\n",
    "</br>https://chatgpt.com/share/670b1688-2560-8000-ae98-96986809c16c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGPTForClassification(\n",
       "  (base_gpt): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(618255, 256)\n",
       "      (wpe): Embedding(128, 256)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-7): 8 x Block(\n",
       "          (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (gelu): NewGELU()\n",
       "            (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=256, out_features=618255, bias=False)\n",
       "  )\n",
       "  (classifier_head): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomGPTForClassification(nn.Module):\n",
    "    def __init__(self, base_gpt, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_gpt = base_gpt  # Pretrained GPT model\n",
    "        self.classifier_head = nn.Linear(base_gpt.config.n_embd, num_classes)\n",
    "        \n",
    "        # Add a config attribute similar to Hugging Face models\n",
    "        self.config = base_gpt.config\n",
    "        # self.config.pad_token_id = self.config.eos_token_id  \n",
    "        # Set pad_token to eos_token\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Pass inputs through the base GPT model\n",
    "        outputs = self.base_gpt(input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs[0]  # Get the last hidden states (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Get the CLS token's hidden state (or you can pool the hidden states differently)\n",
    "        pooled_output = hidden_state[:, -1, :]  # Take the last token's hidden state\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classifier_head(pooled_output)\n",
    "\n",
    "        # If labels are provided, compute the loss (used for training)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss} if loss is not None else {\"logits\": logits}\n",
    "\n",
    "if 'model_mattGPT' not in globals():\n",
    "    model_path=Path(os.getcwd()).joinpath('model')\n",
    "    model_mattGPT = torch.load(model_path.joinpath('model_mattGPT_1.pkl'), weights_only=False)\n",
    "\n",
    "\n",
    "model_mattGPT_classifier = CustomGPTForClassification(model_mattGPT, 2)\n",
    "model_mattGPT_classifier.to(device)\n",
    "# model_mattGPT_classifier.config.pad_token_id = model_mattGPT_classifier.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4) Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    # num_train_epochs=3,\n",
    "    # Doing a really really short training run due to limited time available\n",
    "    max_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5) Finetuning and evaluating MattGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb3479b77b04b68aa64526dacd4f513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x618255 and 256x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer_mattGPT \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_mattGPT_classifier,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m trainer_mattGPT\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2053\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2054\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2055\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2056\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2057\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\transformers\\trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3491\u001b[0m ):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\transformers\\trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 21\u001b[0m, in \u001b[0;36mCustomGPTForClassification.forward\u001b[1;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[0;32m     18\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Take the last token's hidden state\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Classification head\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_head(pooled_output)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# If labels are provided, compute the loss (used for training)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x618255 and 256x2)"
     ]
    }
   ],
   "source": [
    "trainer_mattGPT = transformers.Trainer(\n",
    "    model=model_mattGPT_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer_mattGPT.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the above model is incompatible with the hugging face transformers dataset and data loader.\n",
    "\n",
    "Unfortunately as of writing, I had run out of time to debug how to proceed and cannot perform any further fine-tuning on this model.\n",
    "\n",
    "One possible way forward would be to convert the IMDB dataset into a format that is accepted by the custom dataset constructor above and modify the custom Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation commented out as finetuning failed\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garbage collection thresholds: (700, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del trainer_mattGPT\n",
    "del model_mattGPT_classifier\n",
    "del tokenized_datasets\n",
    "\n",
    "gc.collect()\n",
    "print(\"Garbage collection thresholds:\",gc.get_threshold())\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Test GPT-2 model from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1) Download imdb and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# import torch\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2) Tokenise IMBD dataset using GPT-2 byte-pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3) Define GPT-2 model with classifier head\n",
    "Added fix defining pad_token_id as per solution provided in https://stackoverflow.com/questions/68084302/assertionerror-cannot-handle-batch-sizes-1-if-no-padding-token-is-defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model_gpt2_classifier = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "\n",
    "# set the pad token of the model's configuration\n",
    "model_gpt2_classifier.config.pad_token_id = model_gpt2_classifier.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4) Set training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "   # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # Doing a really really short training run due to limited time available\n",
    "    # max_steps=100,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5) Finetuning and evaluating GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_gpt2 = transformers.Trainer(\n",
    "    model=model_gpt2_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer_gpt2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528b991073ec4a629320be73464d7056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.18055514991283417,\n",
       " 'eval_runtime': 17148.762,\n",
       " 'eval_samples_per_second': 1.458,\n",
       " 'eval_steps_per_second': 0.182,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_gpt2.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6) Evaluate specific sentiment examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentence,model):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    prediction = outputs.logits.argmax(-1).item()\n",
    "    return \"positive\" if prediction == 1 else \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I loved this movie!\"\n",
    "print(\"GPT-2:\",get_sentiment(sentence,model_gpt2_classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Test GPT-2 on list of generated text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "generator = pipeline('text-generation', \n",
    "                     model='gpt2', \n",
    "                     device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'), \n",
    "                     max_length=50,\n",
    "                     truncation=True\n",
    "                     )\n",
    "\n",
    "def return_generated_text(prompt):\n",
    "    print(generator(prompt, pad_token_id=generator.tokenizer.eos_token_id)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shakespearian language prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lord:\n",
      "Rise! My people, conquer the north!\n",
      "\n",
      "Flee; flee, I will make war for you.\n",
      "\n",
      "The great beast was taken, and was taken captive. [Gryffindor died in the Dragon\n"
     ]
    }
   ],
   "source": [
    "return_generated_text('Lord:\\nRise! My people, conquer the north!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APRA Banking regulation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An ADI must have an independent credit risk control unit that is responsible for the assessment of the creditworthiness of an affected individual and must review, in writing, every application to increase the risk that the applicant will default. In 2013 and 2014 an AD\n"
     ]
    }
   ],
   "source": [
    "return_generated_text('An ADI must have an independent credit risk control unit that is responsible for')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed shakespearian language + banking regulation prompt - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING RICHARD III:\n",
      " The key requirements of this Prudential Standard are that Montague hath breathed his last loan-to-value ratio and that the Treasury do not own any of Rothschild's properties because the Federal Reserve has control of its\n"
     ]
    }
   ],
   "source": [
    "return_generated_text('KING RICHARD III:\\n The key requirements of this Prudential Standard are that Montague hath breathed his last loan-to-value ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed shakespearian language + banking regulation prompt - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "We are accounted poor citizens and believe that special purpose vehicles (SPVs) holding securitised assets may be prohibited as the subject of investigation.\n",
      "\n",
      "The SPV will be inspected for any potential illegal conduct.\n"
     ]
    }
   ],
   "source": [
    "return_generated_text('First Citizen:\\nWe are accounted poor citizens and believe that special purpose vehicles (SPVs) holding securitised assets may be')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr Seuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not eat green eggs and ham, instead I use spinach, eggs and tomatoes. I also only buy green potatoes and green peppers (also green and sunflower seeds). There is not much difference in these two. All of my products are made\n"
     ]
    }
   ],
   "source": [
    "return_generated_text('I do not eat green eggs and ham')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Philosophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth is in the eye of the beholder.\"-Charles Darwin\n",
      "\n",
      "\"To think the greatest known example of a true theory of nature and phenomena is that which can only be understood by the greatest number of rational beings.\"-Charles Darwin\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "return_generated_text('Truth is in the eye')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
